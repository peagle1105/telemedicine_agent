{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cac9c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unsloth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbc484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, to_sharegpt\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers.training_args import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5cf9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "token = \"your HF_TOKEN\"  #replace by the token created from hugging face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b600a300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name= model_name,\n",
    "    max_seq_length= max_seq_length,\n",
    "    dtype= dtype,\n",
    "    load_in_4bit= load_in_4bit,\n",
    "    token= token,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912c61b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "from datasets import config\n",
    "\n",
    "# Địa chỉ cache cho các tệp từ Hugging Face Hub (mô hình, tokenizer)\n",
    "cache_dir = config.HF_DATASETS_CACHE\n",
    "\n",
    "print(cache_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5a8745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset_eng_qa = load_dataset(\"lavita/medical-qa-datasets\", \"all-processed\", split = \"train\")\n",
    "dataset_eng_qa_2 = load_dataset(\"eashuu/medical_qa\", split = \"train\")\n",
    "dataset_viet_qa = load_dataset(\"hungnm/vietnamese-medical-qa\", split=\"train\")\n",
    "dataset_viet_diagnosis = load_dataset(\"PB3002/ViMedical_Disease\", split=\"train\")\n",
    "dataset_vipubmed = load_dataset(\"VietAI/vi_pubmed\", split = \"pubmed22\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469a0f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_vipubmed = load_dataset(\"VietAI/vi_pubmed\", split = \"pubmed22\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbf599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_eng_qa.column_names)\n",
    "print(dataset_eng_qa_2.column_names)\n",
    "print(dataset_viet_qa.column_names)\n",
    "print(dataset_viet_diagnosis.column_names)\n",
    "print(dataset_vipubmed.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd404e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "def format_eng_qa(example):\n",
    "    answer = example['output']\n",
    "    output = answer.replace(\"chatbot\", \"telemedicine agent system\")\n",
    "    output = output.replace(\"Chatbot\", \"Telemedicine agent system\")\n",
    "    return {\n",
    "        'instruction': example['instruction'],\n",
    "        'input': example['input'],\n",
    "        'output': output\n",
    "    }\n",
    "\n",
    "def format_viet_qa(example):\n",
    "    # Return dict directly, not wrapped in a list\n",
    "    return {\n",
    "        \"instruction\": \"đọc thông tin sau và giải đáp câu hỏi của bệnh nhân\",\n",
    "        \"input\": example[\"question\"],\n",
    "        \"output\": example[\"answer\"],\n",
    "    }\n",
    "\n",
    "def format_diagnosis(example):\n",
    "    disease = example[\"Disease\"]\n",
    "    answer = (\n",
    "        f\"Dựa trên triệu chứng bạn mô tả, có thể bạn đang mắc bệnh {disease}. \"\n",
    "        \"Tuy nhiên, đây chỉ là đánh giá sơ bộ. Bạn nên đi khám bác sĩ để được chẩn đoán chính xác.\"\n",
    "    )\n",
    "    return {\n",
    "        \"instruction\": \"Dựa vào những triệu chứng mà bệnh nhân mô tả, hãy đưa ra chẩn đoán bệnh.\",\n",
    "        \"input\": example[\"Question\"],\n",
    "        \"output\": answer,\n",
    "    }\n",
    "\n",
    "def format_vipubmed(example):\n",
    "    instruction = \"Hãy đọc văn bản tiếng anh do người dùng cung cấp và dịch sang tiếng việt.\"\n",
    "    en_text = example['en']\n",
    "    input = f\"Bạn hãy giúp tôi dịch văn bản sau đây sang tiếng Việt: {en_text}\"\n",
    "    output = f\"Văn bản sau khi được dịch là:\\n\\t{example['vi']}\"\n",
    "    return {\n",
    "        'instruction': instruction,\n",
    "        'input': input,\n",
    "        'output': output,\n",
    "    }\n",
    "\n",
    "dataset_eng_qa = dataset_eng_qa.map(format_eng_qa)\n",
    "dataset_viet_qa = dataset_viet_qa.map(format_viet_qa)\n",
    "dataset_viet_diagnosis = dataset_viet_diagnosis.map(format_diagnosis)\n",
    "dataset_vipubmed = dataset_vipubmed.map(format_vipubmed)\n",
    "\n",
    "dataset = pd.concat([\n",
    "    pd.DataFrame(dataset_eng_qa),\n",
    "    pd.DataFrame(dataset_eng_qa_2),\n",
    "    pd.DataFrame(dataset_viet_qa),\n",
    "    pd.DataFrame(dataset_viet_diagnosis),\n",
    "    pd.DataFrame(dataset_vipubmed),\n",
    "], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328b2afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = to_sharegpt(\n",
    "    Dataset.from_pandas(dataset),\n",
    "    merged_prompt=\"{instruction}[[\\nYour input is:\\n{input}]]\",\n",
    "    output_column_name=\"output\",\n",
    "    conversation_extension=3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44834721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define chat template\n",
    "chat_template =\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "{SYSTEM}<|eot_id|><start_header_id|>user<|end_header_id|>\n",
    "{INPUT}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "{OUTPUT}<|eot_id|>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7816443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fine-tune model\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules= [\"q_proj\", \"k_proj\", \"v_proj\", \n",
    "                     \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha= 16,\n",
    "    lora_dropout= 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing= \"unsloth\",\n",
    "    random_state= 3407,\n",
    "    use_rslora= False,\n",
    "    loftq_config= None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165af75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(batch):\n",
    "    # batch is a dict: {\"conversations\": [ [...], [...], ... ]}\n",
    "    formatted_texts = []\n",
    "\n",
    "    system_prompt = \"You are a helpful medical assistant. Answer based on the patient's description.\"\n",
    "\n",
    "    for conversation in batch[\"conversations\"]:  # Each `conversation` is a list of {'from': ..., 'value': ...}\n",
    "        text = \"<|begin_of_text|>\"\n",
    "\n",
    "        for msg in conversation:\n",
    "            if not isinstance(msg, dict):\n",
    "                continue\n",
    "            if \"from\" not in msg or \"value\" not in msg:\n",
    "                continue\n",
    "\n",
    "            role = msg[\"from\"]\n",
    "            content = msg[\"value\"].strip()\n",
    "\n",
    "            if role == \"human\":\n",
    "                if \"If you are a doctor\" in content and \"Your input is:\" in content:\n",
    "                    try:\n",
    "                        user_input = content.split(\"Your input is:\\n\", 1)[1]\n",
    "                    except IndexError:\n",
    "                        user_input = content\n",
    "                    text += f\"<|start_header_id|>system<|end_header_id|>\\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n{user_input}<|eot_id|>\"\n",
    "                else:\n",
    "                    text += f\"<|start_header_id|>user<|end_header_id|>\\n{content}<|eot_id|>\"\n",
    "            elif role == \"gpt\":\n",
    "                text += f\"<|start_header_id|>assistant<|end_header_id|>\\n{content}<|eot_id|>\"\n",
    "\n",
    "        formatted_texts.append(text)\n",
    "\n",
    "    return formatted_texts  # List of strings, one per example\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    processing_class= tokenizer,\n",
    "    train_dataset= dataset,\n",
    "    peft_config= model.peft_config,\n",
    "    formatting_func = formatting_func,\n",
    "    args= TrainingArguments(\n",
    "        output_dir= \"./output\",\n",
    "        per_device_train_batch_size= 2,\n",
    "        gradient_accumulation_steps= 4,\n",
    "        warmup_steps= 5,\n",
    "        max_steps= 60,\n",
    "        num_train_epochs= 1,\n",
    "        learning_rate= 2e-4,\n",
    "        fp16= not is_bfloat16_supported(),\n",
    "        bf16= is_bfloat16_supported(),\n",
    "        logging_steps= 1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay= 0.01,\n",
    "        lr_scheduler_type= \"linear\",\n",
    "        seed= 3407,\n",
    "    )\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b430ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save_pretrained(\"../models/fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"../models/fine_tuned_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
