from unsloth import FastLanguageModel
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.embeddings import GPT4AllEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_huggingface import HuggingFacePipeline
import torch
from transformers.pipelines import pipeline
import os

# --- C·∫•u h√¨nh ---
model_name = "./models/fine_tuned_model"
vector_db_path = "./agents/vector_db_store/rag_vector_db"
embedding_model_path = "/mnt/d/HOPT/Agent_build/Telemed_agent/models/all-MiniLM-L6-v2-f16.gguf"
max_seq_length = 2048
load_in_4bit = True

# --- B∆∞·ªõc 1: Load LLM ƒë√£ fine-tune t·ª´ Unsloth ---
def load_llm():
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=model_name,
        max_seq_length=max_seq_length,
        load_in_4bit=load_in_4bit,
        dtype=None,
        device_map="auto",
    )

    # √Åp d·ª•ng inference
    FastLanguageModel.for_inference(model)

    # T·∫°o pipeline v·ªõi Hugging Face
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto",
        max_new_tokens=256,
        temperature=0.7,
        top_p=0.9,
        repetition_penalty=1.1,
    )

    # Wrap th√†nh LLM cho LangChain
    llm = HuggingFacePipeline(pipeline=pipe)
    return llm, tokenizer

# --- B∆∞·ªõc 2: T·∫°o ho·∫∑c ƒë·ªçc Vector DB ---
from langchain_huggingface import HuggingFaceEmbeddings

def read_vector_db():
    # ‚úÖ D√πng HuggingFace thay v√¨ GPT4All
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"}
    )

    db = FAISS.load_local(
        vector_db_path,
        embeddings,
        allow_dangerous_deserialization=True,
    )
    return db

# --- B∆∞·ªõc 3: T·∫°o prompt cho RAG ---
def create_prompt():
    template = """
B·∫°n l√† m·ªôt tr·ª£ l√Ω y t·∫ø th√¥ng minh, c√≥ nhi·ªám v·ª• tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p.
H√£y l√†m theo c√°c b∆∞·ªõc sau:
1. ƒê·ªçc k·ªπ th√¥ng tin trong ph·∫ßn "Context".
2. X√°c ƒë·ªãnh c√°c ƒëi·ªÉm ch√≠nh li√™n quan ƒë·∫øn c√¢u h·ªèi.
3. T·ªïng h·ª£p v√† suy lu·∫≠n ƒë·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß, r√µ r√†ng.
4. N·∫øu kh√¥ng ch·∫Øc ch·∫Øn, h√£y n√≥i "T√¥i kh√¥ng bi·∫øt d·ª±a tr√™n t√†i li·ªáu ƒë√£ cho." ‚Äì ƒê·ª´ng b·ªãa.

Context:
{context}

C√¢u h·ªèi:
{question}

H√£y suy nghƒ© v√† tr·∫£ l·ªùi:
"""
    return PromptTemplate(template=template, input_variables=["context", "question"])

# --- B∆∞·ªõc 4: T·∫°o chu·ªói RAG ---
def create_qa_chain(llm, prompt, db):
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=db.as_retriever(search_kwargs={"k": 3}),
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt},
    )
    return qa_chain

# --- B∆∞·ªõc 5: Chat t∆∞∆°ng t√°c ---
def chat():
    print("üéØ ƒêang t·∫£i m√¥ h√¨nh v√† c∆° s·ªü d·ªØ li·ªáu...")
    llm, tokenizer = load_llm()
    db = read_vector_db()
    prompt = create_prompt()
    qa_chain = create_qa_chain(llm, prompt, db)

    print("‚úÖ S·∫µn s√†ng! Nh·∫≠p 'exit' ƒë·ªÉ tho√°t.\n")

    while True:
        question = input("B·∫°n h·ªèi: ").strip()
        if question.lower() == "exit":
            print("T·∫°m bi·ªát!")
            break
        if not question:
            continue
        # G·ªçi RAG chain
        try:
            result = qa_chain.invoke({"query": question})
            print(f"Tr·∫£ l·ªùi: {result['result']}\n")
        except Exception as e:
            print(f"L·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {e}")

if __name__ == "__main__":
    chat()